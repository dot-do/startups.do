---
name: EADraft — AI Finding Aid Generator for ArchivesSpace/EAD
slug: eadraft
service:
  title: Finding Aid Drafting to EAD/ArchivesSpace
  description: >-
    Transform inventories and notes into EAD-compliant finding aids with
    biographical/scope notes and controlled access terms.
  targetUsers:
    - Archives
    - Special collections
    - Local history centers
  triggers:
    - New accession or processed collection
    - Legacy guide modernization
  inputs:
    - Box/folder lists (CSV/Excel)
    - Accession records
    - Biog/admin history notes
    - Local subject/name policies
  steps:
    - Parse and normalize container lists
    - Generate series/file hierarchies
    - Draft biographical/scope notes
    - Suggest controlled access headings
    - Export EAD; validate
    - Post to ArchivesSpace via API
    - Human archivist review
  tools:
    - Python/pandas
    - OpenAI API (summaries/headings)
    - ArchivesSnake (ArchivesSpace API)
    - EAD schema validators
  outputs:
    - EAD XML and PDF
    - Access headings with URIs
    - Change/QC report
  pricingModel:
    - Per collection (tiers by size)
    - Optional ongoing updates retainer
  humanInLoop: true
  feasibility:
    remoteOnLaptop: 5
    modelCapability: 4
    overall: 4.3
  risks:
    - Hallucinated context if notes are sparse
    - PII exposure in notes
    - Arrangement choices misaligned with practice
  dependencies:
    - ArchivesSpace or EAD ingest endpoint
    - Local descriptive standards (DACS)
    - Controlled vocabulary sources
leanCanvas:
  problem:
    - >-
      Manual drafting of EAD-compliant finding aids is slow and costly (often
      40–80 staff hours for a 10–20 linear-foot collection, even under MPLP),
      creating persistent backlogs.
    - >-
      Archivists must synthesize scattered inventories, donor notes, and
      spreadsheets into structured EAD with DACS fields; quality and consistency
      vary by staff expertise and turnover.
    - >-
      Authority control (LCNAF, LCSH/FAST, VIAF, SNAC) is tedious to reconcile
      by hand, leading to inconsistent controlled access terms and weaker
      discovery.
    - >-
      ArchivesSpace imports fail or create messy trees when inputs are
      inconsistent; EAD validation errors require expert troubleshooting.
    - >-
      Many small and mid-size institutions lack capacity for extensive
      description projects and struggle to secure sustainable funding for
      contracting out.
    - >-
      Existing tools focus on data entry and storage; few help with first-draft
      narrative notes (biographical/historical, scope and content) or
      arrangement suggestions.
    - >-
      PII and donor restrictions are hard to detect during description,
      increasing legal and ethical risk.
  solution:
    - >-
      Automated ingestion of Word/Google Docs, Excel/CSV, PDFs, and email
      inventories; OCR for scanned inventories with table detection.
    - >-
      AI parsing to detect series/subseries, container lists, dates, extents,
      and arrangement; confidence scores on each element.
    - >-
      Drafting of DACS-compliant fields: biographical/historical notes, scope
      and content, preferred citation, custodial history, conditions governing
      access/use.
    - >-
      Authority reconciliation to LCNAF, LCSH/FAST, VIAF, SNAC, AAT, and
      GeoNames with top-ranked matches and human confirmation.
    - >-
      Standards validation and remediation: EAD 2002/EAD3 schema checks;
      required element prompts; normalization of dates/extents; language/script
      tagging.
    - >-
      ArchivesSpace integration: create/update resources, components, agents,
      and subjects via API; diff view of proposed vs. existing records.
    - >-
      PII and sensitive-content detection (names, SSNs, health, financial
      tokens) with redaction suggestions and restriction note templates.
    - >-
      Batch processing and templating for multi-collection projects; reusable
      institution-level style and encoding profiles.
    - >-
      Reporting and QA dashboards: acceptance rates, validation pass/fail,
      authority match precision/recall, average time saved per collection.
    - >-
      On-prem/VPC option for sensitive collections; SSO (SAML/OIDC); audit logs
      for compliance.
  uniqueValueProp: >-
    Turn messy inventories and notes into EAD/ArchivesSpace-ready finding aids
    in hours, not weeks—validated against EAD/DACS, reconciled with authority
    files, and reviewable in a human-in-the-loop UI that measurably reduces
    backlog and boosts discovery.
  unfairAdvantage: >-
    A proprietary training corpus of aligned inventory-to-EAD pairs and
    gold-standard authority links from partner institutions, combined with deep
    ArchivesSpace integration and an EAD validator tuned to real institutional
    practice; procurement-ready (SOC 2, accessibility), with on-prem/VPC
    deployment for sensitive archives.
  customerSegments:
    - >-
      Academic libraries and university archives (ARL, regional universities,
      community colleges with archives).
    - >-
      Public libraries with local history collections and special collections
      divisions.
    - 'State, municipal, and federal government archives and records centers.'
    - 'Museums with archival holdings (art, science, history museums).'
    - Historical societies and cultural heritage nonprofits.
    - Religious orders and corporate archives with compliance needs.
    - >-
      Digitization vendors and consultants seeking to add AI description as a
      white-label service.
    - >-
      Library consortia and shared-services hubs (e.g., LYRASIS members, Orbis
      Cascade, BTAA).
  channels:
    - >-
      Founding pilot cohort (10–20 ArchivesSpace member institutions via LYRASIS
      outreach) with discounted pricing and co-authored case studies.
    - >-
      Conference demos and workshops: SAA Annual Meeting, ALA Core, DLF Forum,
      RBMS, MARAC, NAGARA, Code4Lib.
    - >-
      Targeted webinars with consortia (e.g., California Digital Library
      network, Orbis Cascade) and regional archives associations (SCA, NEA,
      MAC).
    - >-
      Content marketing: “From Inventory to EAD in a Day” playbook, ROI
      calculator (hours saved vs. cost), sample datasets and videos, and EAD
      validation checklist.
    - >-
      Partner/reseller channel: digitization vendors and archival consultants
      bundling EADraft in project quotes; revenue share.
    - >-
      Open-source goodwill: release our EAD validator ruleset and an
      ArchivesSpace import helper plugin to drive inbound leads.
    - >-
      Grants alignment: NEH Humanities Collections/Ref, IMLS National
      Leadership; provide boilerplate language so institutions can include
      EADraft as a budget line.
    - >-
      Customer reference program: publish 3–5 success stories showing 60–80%
      time savings and improved authority control precision.
  revenueStreams:
    - >-
      Annual subscriptions by tier (Starter $3k, Professional $12k, Enterprise
      $30k+) with usage quotas (e.g., number of archival objects or linear feet
      processed).
    - >-
      Usage-based overage: per-folder or per-linear-foot pricing (e.g., $10–$30
      per linear foot, volume discounts).
    - >-
      Professional services: onboarding/migration, custom templates, local
      authority mapping, bulk backlog projects.
    - >-
      On-prem/VPC license with premium support and maintenance (priced
      annually).
    - >-
      Training and certification for staff and student workers; paid workshops
      for consortia.
    - >-
      White-label offering for digitization vendors and consultants (wholesale
      pricing).
    - >-
      Implementation grants support: proposal writing assistance packaged with
      subscription (success-fee or fixed).
  costStructure:
    - >-
      Model inference costs (LLM/API or hosted model GPUs), vector search, and
      prompt orchestration infrastructure.
    - >-
      OCR/HTR processing costs for scanned inventories; table/structure
      extraction engines.
    - >-
      Secure cloud hosting (VPC), storage, backups, encryption, and network
      egress; on-prem deployment support.
    - >-
      Engineering salaries (NLP/ML, backend, integrations), archival domain
      experts/editors, UX, QA.
    - >-
      Customer success, onboarding, and support; documentation and training
      content.
    - >-
      Compliance and security (SOC 2 Type II, penetration testing, SSO),
      accessibility audits, legal.
    - >-
      Authority data licensing/compliance where applicable (Getty AAT/ULAN
      terms, data mirrors, rate-limit proxies).
    - >-
      Conference sponsorships, webinars, and content marketing; partner
      enablement.
    - 'Insurance (E&O, cyber), finance, and administrative overhead.'
    - >-
      Data acquisition/annotation: licensing or partnering for training corpora;
      human labeling for gold-standard sets.
  keyMetrics:
    - >-
      Average time-to-first-draft per collection; goal: reduce by 60–80% vs.
      baseline manual process.
    - 'EAD validation pass rate at first export; goal: ≥95%.'
    - >-
      Authority matching quality: top-1 precision ≥90% and recall ≥85% on
      internal benchmark; track false positives by type.
    - >-
      Human edit rate: percent of auto-generated fields edited by staff; goal:
      <20% for scope notes and <10% for dates/extents after 3 months of tuning.
    - >-
      Backlog throughput: collections processed per month per institution;
      target uplift 3–5x.
    - 'ArchivesSpace sync success rate (no errors/warnings); goal: ≥98%.'
    - >-
      PII detection precision/recall (measured on seeded test sets); target:
      ≥0.9 precision.
    - 'Pilot-to-paid conversion rate; goal: ≥50%.'
    - '12-month logo retention; goal: ≥92%.'
    - 'NPS; goal: ≥40 within 6 months.'
    - 'Gross margin; goal: ≥70% by Year 2.'
    - >-
      ARPA (average revenue per account) and net revenue retention; goal: NRR
      ≥110%.
    - >-
      Support tickets per 100 finding aids processed; goal: ≤3 with median
      time-to-resolution <1 business day.
storyBrand:
  character: >-
    Archivists and librarians who want to convert box lists, inventories, and
    accession notes into publishable, standards-compliant finding aids quickly.
  problem: >-
    External: Manual EAD/ArchivesSpace encoding is slow, inconsistent, and
    bottlenecked by staff time. Internal: Teams feel overwhelmed by backlogs and
    worried about errors and nonstandard description. Philosophical: Cultural
    heritage materials deserve accurate, discoverable description without
    marathon editing.
  guide: >-
    We understand archival workflows—built with archivists, trained on archival
    standards and vocabularies (EAD, DACS, LCSH, LCNAF). Proven on real
    collections with human-in-the-loop QA, privacy-by-design, and clear audit
    trails.
  plan: >-
    1) Upload inventories, notes, and box lists. 2) Our AI drafts EAD-compliant
    finding aids with biographical/scope notes and controlled access terms;
    flags ambiguities for review. 3) You review, edit, and approve; export to
    EAD XML or push to ArchivesSpace.
  callToAction: >-
    Direct: Book a 30-minute demo or start a 10-record pilot. Transitional: Try
    a free sample conversion from one inventory.
  success: >-
    Shrink backlog, publish consistent EAD records, improve discovery, meet
    standards, accelerate accession-to-access, free staff for higher-value work,
    and delight researchers and stakeholders.
  failure: >-
    Backlog grows, records remain inconsistent, materials stay hidden, reporting
    and grant opportunities suffer, and staff burn out maintaining manual
    processes.
landingPage:
  hero:
    title: Finding Aid Drafting to EAD/ArchivesSpace
    subtitle: >-
      Transform inventories and notes into EAD-ready finding aids with
      biographical/scope notes and controlled access terms.
    ctaText: Book a demo
    ctaHref: '#request-demo'
  problem:
    - Manual drafting is slow and backlogs grow.
    - Standards (DACS/EAD) are hard to apply consistently.
    - Authority control and reconciliation consume hours.
    - 'Dates, extents, and containers require tedious normalization.'
    - 'Legacy inventories live in Word, PDFs, and spreadsheets.'
    - Clean ArchivesSpace imports demand precise mapping and QA.
  solution:
    - >-
      Upload inventories and notes; receive a standards-aligned draft finding
      aid.
    - Automatic biographical and scope/content notes from your materials.
    - Entity extraction with suggested controlled access terms.
    - Multilevel components and container lists built from lists and headings.
    - Authority reconciliation suggestions (select and confirm).
    - Validation and QA checks before export.
    - Export clean EAD XML or push directly to ArchivesSpace.
  features:
    - DACS-aligned templates for core elements.
    - EAD2002 and EAD3 output with schema validation.
    - ArchivesSpace mapping and API ingest.
    - 'Controlled vocab suggestions: LCNAF, LCSH, AAT, FAST, VIAF.'
    - Date parsing and normalization (inclusive/bulk; ISO 8601).
    - Extent calculation and physical description scaffolding.
    - Series/subseries detection and arrangement suggestions.
    - Name/place/subject extraction with disambiguation and confidence scores.
    - Sensitive term and PII flagging for review.
    - 'Rights, access, use, citation, acquisition, and processing note scaffolds.'
    - Language and script detection.
    - 'Digital object linking (URIs, file refs, thumbnails).'
    - 'Import from DOCX, PDF, CSV, and spreadsheets.'
    - 'QA rules for required elements, orphan components, and container gaps.'
    - 'Versioning, audit trail, and side-by-side diffs.'
    - Human-in-the-loop approvals; transparent field-level mapping.
    - Private cloud or on‑prem deployment; encryption in transit and at rest.
    - Your data isn’t used to train shared models unless you opt in.
  steps:
    - Upload files or drag-and-drop inventories and notes.
    - Choose standards profile and preferred authority sources.
    - 'Review AI-drafted notes, components, and containers; edit inline.'
    - Resolve names/subjects with authority suggestions.
    - Run validation and QA checks; address flagged issues.
    - Export EAD XML or send directly to ArchivesSpace.
    - Download a change report and archive the approved version.
---
# EADraft — AI Finding Aid Generator for ArchivesSpace/EAD

Industry: Libraries and Archives
Service: Finding Aid Drafting to EAD/ArchivesSpace
