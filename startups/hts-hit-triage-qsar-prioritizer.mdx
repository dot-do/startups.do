---
name: HTS Hit Triage & QSAR Prioritizer
slug: hts-hit-triage-qsar-prioritizer
naics:
  primary: '541714'
  occupations: []
service:
  title: HTS Hit Triage & QSAR Prioritizer
  description: >-
    Feature engineering, model training, and applicability-domain aware
    prioritization for high-throughput screening outcomes without de novo
    design.
  targetUsers:
    - Medicinal chemists
    - Screening groups
    - Compound management
  triggers:
    - Primary/secondary screen complete
    - Hit list requires prioritization
    - Assay drift suspected
  inputs:
    - Compound IDs/structures (SMILES/SDF)
    - Assay readouts and metadata
    - Assay interference/bad actor rules
  steps:
    - Sanitize and standardize structures; remove duplicates/salts
    - Compute descriptors/fingerprints; flag PAINS/reactive motifs
    - Split data with leakage checks; train baseline and ensemble models
    - Calibrate probabilities; assess applicability domain and uncertainty
    - >-
      Cluster hits; propose diverse, high-confidence subsets; flag potential
      artifacts
    - 'Produce explainability (e.g., SHAP) and model cards; package for re-use'
  tools:
    - RDKit
    - scikit-learn/XGBoost/LightGBM
    - DeepChem (optional)
    - SHAP
    - DVC for model versioning
  outputs:
    - Ranked compound lists with confidence
    - Model artifacts and documentation
    - Clustering visuals and AD metrics
  pricingModel:
    - Per-screen project fee
    - Success fee tied to follow-up confirmation (optional)
    - Subscription for continuous model updates
  humanInLoop: true
  feasibility:
    remoteOnLaptop: 4
    modelCapability: 4.5
    overall: 4.3
  risks:
    - Assay artifacts and confounding
    - Overfitting from imbalance
    - Regulatory/IP constraints on compound data
  dependencies:
    - Secure compound/assay data access
    - Cheminformatics licensing if commercial tools are requested
    - Agreement on triage criteria
leanCanvas:
  problem:
    - >-
      Primary HTS hit lists include high artifact rates (PAINS/aggregators,
      autofluorescence, plate/batch effects), leading to 40–80% false positives
      in confirmatory assays and wasted budget/time.
    - >-
      QSAR models trained on heterogeneous HTS outcomes often fail to
      generalize; lack of uncertainty quantification and applicability-domain
      (AD) awareness causes overconfident, wrong prioritizations.
    - >-
      Teams lack standard, reproducible feature engineering and plate-level QC
      pipelines; manual triage is slow and inconsistent across projects.
    - >-
      Follow-up capacity is constrained; without data-driven enrichment,
      confirmatory testing costs and timelines balloon (weeks to months).
    - >-
      Integration gaps with LIMS/ELN and chemistry toolchains create friction
      and rework; auditability and traceability are insufficient for decision
      reviews.
    - >-
      Limited in-house ML/cheminformatics expertise to build and maintain robust
      models with drift monitoring, AD, and active learning at scale.
  solution:
    - >-
      Automated feature engineering for HTS outcomes: chemical
      descriptors/fingerprints, plate/batch covariates, assay-context features,
      artifact/PAINS flags.
    - >-
      Model training with uncertainty: conformal prediction and calibrated
      probabilities to rank by both predicted effect and confidence.
    - >-
      Applicability-domain aware prioritization: distance- and density-based AD
      plus domain-specific covariates to exclude out-of-domain recommendations.
    - >-
      Plate- and batch-effect correction: well/edge effects, signal drift,
      control normalization, and blind-spot detection to reduce artifacts before
      modeling.
    - >-
      Active learning for confirmatory selection: propose minimal, diverse,
      high-value subsets to maximize enrichment per confirmation dollar.
    - >-
      Seamless integration: connectors to common LIMS/ELN (e.g., Dotmatics,
      Benchling), SD file workflows, and cloud/on‑prem deployment with full
      audit trails.
    - >-
      Monitoring and model governance: drift detection, periodic re-training,
      versioning, and traceable decisions for internal reviews and partner
      reporting.
  uniqueValueProp: >-
    Increase confirmatory hit rate and reduce wasted assays with
    applicability-domain aware QSAR triage, plate/QC-aware feature engineering,
    and uncertainty-calibrated prioritization—without de novo design or IP risk.
  unfairAdvantage: >-
    A production-grade, applicability-domain aware triage stack that jointly
    models chemical, plate, and assay-context features with conformal
    uncertainty—validated on public HTS sets and packaged with integrations and
    governance so teams realize measurable enrichment within one campaign.
  customerSegments:
    - >-
      Mid-size biopharma (50–1,000 R&D FTE) running in-house or partnered HTS
      campaigns seeking higher confirmatory hit quality.
    - >-
      Large pharma therapeutic area teams needing scalable, AD-aware
      prioritization across multiple screening modalities.
    - >-
      CROs/CDMOs offering HTS and secondary screening that want to boost hit
      quality and throughput for sponsors.
    - >-
      Academic screening centers and non-profit consortia needing standardized
      triage and reproducible QSAR for diverse assays.
    - >-
      AI-native or virtual biotechs that do not do de novo design but need
      robust triage and prioritization of purchased libraries.
  channels:
    - >-
      Direct enterprise sales to mid-size biopharma therapeutic units and CRO BD
      teams with 6–8 week paid pilots.
    - >-
      Partnerships with CROs to embed triage as a value-add service; revenue
      share or bundled pricing for sponsors.
    - >-
      Cloud marketplaces (AWS/GCP) listings to ease procurement for VPC
      deployments.
    - >-
      Co-marketing with LIMS/ELN vendors and cheminformatics tool providers
      (Benchling, Dotmatics, ChemAxon) via connectors and joint webinars.
    - >-
      Conference presence and workshops at SLAS, BIO, ACS, ELRIG; publish
      comparative case studies with independent screening centers.
    - >-
      Founder-led thought leadership: benchmarking on public HTS datasets
      (PubChem, ChEMBL) and reproducible notebooks for credibility.
    - >-
      Targeted outbound to teams with recent HTS publications/grants; ABM
      campaigns based on disclosed assay modalities and targets.
  revenueStreams:
    - >-
      Subscription SaaS (annual): Starter $60–100k (single team/assay), Pro
      $120–200k (multi-assay, integrations), Enterprise $250–500k (multi-site,
      on‑prem/VPC, SSO).
    - >-
      Usage-based compute/storage overage for large campaigns (e.g., $0.20–0.40
      per vCPU-hour, $20–35/TB-month).
    - >-
      Professional services: onboarding, custom integrations, bespoke model
      development ($180–250/hour or scoped SOW).
    - >-
      Paid pilots: $50–75k for 6–8 weeks with predefined success criteria and
      conversion credits to subscription.
    - >-
      CRO embedded model licensing: annual platform fee plus per-campaign seats;
      optional success-linked bonus based on confirmed hit uplift.
    - >-
      Training and enablement packages: $5–15k per cohort for internal
      upskilling and governance workshops.
  costStructure:
    - >-
      Salaries: ML engineers/cheminformaticians, data scientists, software
      engineers, solutions architects, sales/CS, security/compliance.
    - >-
      Cloud infrastructure: compute for training/inference, storage, networking,
      observability; CI/CD pipelines and test environments.
    - >-
      Security and compliance: SOC 2 Type II, pen tests, VPC hardening, audit
      logging, key management.
    - >-
      Licenses and data: commercial descriptor/toolkits (if resold),
      cheminformatics components, code scanning tools.
    - >-
      Sales and marketing: conferences, pilots (COGS compute + staff), content,
      solution engineering time.
    - 'Legal and insurance: MSAs, DPAs, IP, E&O/cyber insurance.'
    - >-
      R&D: method development (conformal/AD advances), benchmarking,
      integrations, and productization of active learning workflows.
  keyMetrics:
    - >-
      Hit rate uplift in confirmatory assays vs. baseline triage (target +2–4x
      EFx10; +15–35% absolute precision at fixed recall).
    - Cost per qualified hit (target 25–50% reduction within 2 campaigns).
    - >-
      Time-to-decision from primary HTS to confirmatory selection (target 30–50%
      reduction).
    - >-
      Pilot success scorecard: predefined AUROC/PR AUC thresholds, enrichment
      vs. negative controls, and AD coverage percentage.
    - >-
      Pilot-to-subscription conversion rate (target ≥50%); time-to-first-value
      ≤2 weeks; onboarding cycle ≤30 days.
    - >-
      Net revenue retention (target ≥120% with expansion to additional
      assays/teams); logo retention ≥90%.
    - >-
      Compute efficiency: $ per 1M compounds triaged; inference throughput
      (compounds/min) with AD enabled.
    - >-
      Data ingestion SLA (target <48 hours from data handoff); integration SLA
      compliance (>95%).
    - >-
      Model drift alerts per quarter and retrain cadence adherence (target <2
      unplanned retrains/quarter).
storyBrand:
  character: >-
    Biotech R&D and screening teams who must rapidly triage HTS hits and choose
    confirmatory candidates with confidence and traceability—without de novo
    design.
  problem: >-
    - External: HTS yields thousands of noisy hits with artifacts, PAINS, and
    batch effects while confirmatory budget and time are limited.

    - Internal: Uncertainty and decision fatigue; skepticism of black-box AI;
    pressure to justify selections.

    - Philosophical: Hit selection should be data-driven, transparent, and
    reproducible so good science isn’t derailed by noise.
  guide: >-
    We understand you can’t afford to chase noise or black-box recommendations.
    HTS Hit Triage & QSAR Prioritizer applies proven QSAR, applicability-domain
    estimation, and calibrated uncertainty with transparent validation. Designed
    for NAICS 541714 workflows, we emphasize reproducibility, clear
    documentation, and seamless integration with your cheminformatics stack.
  plan: >-
    Process plan:

    - Scope & intake: objectives, assay context, controls, plate maps,
    structures, metadata.

    - Curation & features: standardize/annotate compounds, artifact filters,
    plate normalization, rich descriptors/fingerprints.

    - Train & validate: cross-validated QSAR/ML, calibration,
    applicability-domain mapping, bias checks.

    - Prioritize & deliver: rank by confirmability, diversity, and AD coverage;
    provide report + notebook/API.


    Agreement plan:

    - NDA + data governance; you own your data, models, and IP.

    - Secure deployment (on-prem or private cloud).

    - Timeboxed pilot with clear success criteria and acceptance metrics.
  callToAction: |-
    - Direct: Start a 4-week pilot on your latest HTS campaign.
    - Transitional: Request a sample prioritization report and methods brief.
  success: >-
    - Confident, explainable hit lists that respect applicability domain and
    uncertainty.

    - Reduced spend on false positives; more diverse, mechanism-relevant
    chemistry entering confirmation.

    - Faster path from screen to SAR and lead hypotheses.

    - Audit-ready, reproducible decisions that build stakeholder trust and
    momentum.
  failure: >-
    - Budget burned on artifacts and me-too chemotypes.

    - Weeks lost to manual triage and debate; missed competitive windows.

    - Opaque choices erode confidence; promising hits are overlooked and
    programs stall.
landingPage:
  hero:
    title: HTS Hit Triage & QSAR Prioritizer
    subtitle: >-
      AI triage for high‑throughput screens. We engineer features, train
      calibrated QSARs, and rank hits with applicability‑domain awareness—no de
      novo design required.
    ctaText: Book a demo
    ctaHref: '#demo'
  problem:
    - HTS yields thousands of hits with unclear confirm-worthiness
    - Plate effects and artifacts inflate false positives
    - QSARs break when chemistry drifts outside training space
    - No uncertainty estimates → costly follow‑ups
    - Manual feature engineering is slow and inconsistent
    - Fragmented data formats slow decision‑making
  solution:
    - 'End‑to‑end triage that turns raw HTS into ranked, actionable hit lists'
    - >-
      Applicability‑domain aware QSAR with calibrated probabilities and abstain
      outside domain
    - >-
      Automated, assay‑aware feature engineering across fingerprints,
      descriptors, and GNN embeddings
    - >-
      Robust data QC: plate normalization, replicate handling, and artifact
      flagging
    - Transparent model cards and audit‑ready reports for every run
    - Fast export to CSV/SDF and seamless handoff to confirmatory assays
  features:
    - >-
      Automated featurization: physicochemical descriptors, circular
      fingerprints, learned graph embeddings
    - >-
      Data hygiene: control-based plate correction, replicate consensus,
      PAINS/aggregator flags
    - >-
      Model zoo: RF/XGBoost, elastic nets, GNNs with nested CV and
      hyperparameter search
    - >-
      Calibration and risk control via conformal prediction; uncertainty and
      abstention out of domain
    - >-
      Applicability domain via leverage, embedding distance, and ensemble
      disagreement
    - >-
      Multi‑objective ranking by activity, novelty, diversity, and simple ADMET
      proxies
    - Transfer learning across related assays to boost signal with minimal data
    - >-
      Interactive thresholds with expected confirmation rate and cost
      projections
    - 'Reproducible pipelines with versioned data, features, and seeds'
    - 'Simple integration: SMILES/SDF/SD/CSV in; CSV/SDF out'
  steps:
    - Ingest HTS results and structures; define endpoints and assay metadata
    - >-
      QC and normalize: plate/control corrections, replicate reconciliation,
      artifact checks
    - Engineer features and build featurization ensembles
    - 'Train, tune, and calibrate models with cross‑validation and holdouts'
    - Score compounds with uncertainty and applicability‑domain tags
    - Prioritize and export ranked pick lists with cutoffs and rationale
---

# HTS Hit Triage & QSAR Prioritizer

```typescript
// Core business workflow types
interface Lead {
  id: string;
  company: string;
  contactName: string;
  email: string;
  htsVolume: number;
  assayTypes: string[];
  currentTriageMethod: 'manual' | 'basic-qsar' | 'none';
  painPoints: string[];
  budget: number;
  timeline: string;
}

interface Customer {
  id: string;
  company: string;
  tier: 'starter' | 'pro' | 'enterprise';
  contractValue: number;
  onboardingStatus: 'pending' | 'in-progress' | 'complete';
  integrations: string[];
}

interface HTSCampaign {
  id: string;
  customerId: string;
  compounds: number;
  assayType: string;
  plateCount: number;
  status: 'uploaded' | 'processing' | 'triaged' | 'delivered';
  confidenceThreshold: number;
}

interface TriageResults {
  campaignId: string;
  rankedCompounds: CompoundScore[];
  modelMetrics: ModelMetrics;
  applicabilityDomain: ADMetrics;
  reportUrl: string;
}

interface CompoundScore {
  compoundId: string;
  smiles: string;
  predictedActivity: number;
  confidence: number;
  inDomain: boolean;
  painsFlags: string[];
  rank: number;
}

interface ModelMetrics {
  auroc: number;
  precision: number;
  recall: number;
  calibrationError: number;
  domainCoverage: number;
}

interface ADMetrics {
  totalCompounds: number;
  inDomainCount: number;
  coverage: number;
  leverageThreshold: number;
}

// Customer Acquisition Workflows
export async function acquireCustomer(lead: Lead): Promise<Customer> {
  const qualifiedLead = await qualifyLead(lead);
  const pilot = await proposePilot(qualifiedLead);
  const contract = await negotiateContract(pilot);
  return await onboardCustomer(contract);
}

export async function qualifyLead(lead: Lead): Promise<Lead> {
  // Qualify based on HTS volume, budget, and technical fit
  if (lead.htsVolume < 10000) {
    throw new Error('Lead does not meet minimum HTS volume threshold');
  }
  
  if (lead.budget < 50000) {
    throw new Error('Lead budget insufficient for pilot engagement');
  }
  
  // Score lead based on pain points and current methods
  const qualificationScore = calculateLeadScore(lead);
  if (qualificationScore < 0.7) {
    throw new Error('Lead qualification score too low');
  }
  
  return { ...lead, qualified: true };
}

export async function proposePilot(lead: Lead): Promise<PilotProposal> {
  const pilotScope = await definePilotScope(lead);
  const successCriteria = await defineSuccessCriteria(lead);
  const pricing = await calculatePilotPricing(pilotScope);
  
  return {
    leadId: lead.id,
    scope: pilotScope,
    successCriteria,
    pricing,
    duration: '6-8 weeks',
    deliverables: [
      'Triaged hit list with confidence scores',
      'Model performance report',
      'Applicability domain analysis',
      'Integration assessment'
    ]
  };
}

export async function negotiateContract(pilot: PilotProposal): Promise<Contract> {
  const terms = await generateContractTerms(pilot);
  const msa = await prepareMSA(pilot.leadId);
  const dpa = await prepareDataProcessingAgreement(pilot.leadId);
  
  return {
    pilotId: pilot.id,
    terms,
    msa,
    dpa,
    conversionCredits: pilot.pricing * 0.8, // 80% pilot cost credited to subscription
    status: 'pending-signature'
  };
}

export async function onboardCustomer(contract: Contract): Promise<Customer> {
  const customer = await createCustomerAccount(contract);
  await setupSecureEnvironment(customer);
  await configureIntegrations(customer);
  await scheduleKickoffMeeting(customer);
  
  return {
    ...customer,
    onboardingStatus: 'in-progress',
    expectedGoLive: new Date(Date.now() + 14 * 24 * 60 * 60 * 1000) // 2 weeks
  };
}

// Product Development Processes
export async function developTriageModel(campaign: HTSCampaign): Promise<TriageModel> {
  const cleanedData = await preprocessHTSData(campaign);
  const features = await engineerFeatures(cleanedData);
  const model = await trainQSARModel(features);
  const calibratedModel = await calibrateModel(model);
  const validatedModel = await validateModel(calibratedModel);
  
  return validatedModel;
}

export async function preprocessHTSData(campaign: HTSCampaign): Promise<CleanedData> {
  // Plate normalization and artifact removal
  const plateNormalized = await normalizePlateEffects(campaign);
  const artifactsRemoved = await flagArtifacts(plateNormalized);
  const duplicatesRemoved = await removeDuplicates(artifactsRemoved);
  const standardizedStructures = await standardizeStructures(duplicatesRemoved);
  
  return standardizedStructures;
}

export async function engineerFeatures(data: CleanedData): Promise<FeatureMatrix> {
  const molecularDescriptors = await computeMolecularDescriptors(data);
  const fingerprints = await computeFingerprints(data);
  const plateFeatures = await extractPlateFeatures(data);
  const assayFeatures = await extractAssayFeatures(data);
  
  return combineFeatures([
    molecularDescriptors,
    fingerprints,
    plateFeatures,
    assayFeatures
  ]);
}

export async function trainQSARModel(features: FeatureMatrix): Promise<QSARModel> {
  const splitData = await performTrainTestSplit(features);
  const hyperparameters = await optimizeHyperparameters(splitData.train);
  const model = await trainEnsembleModel(splitData.train, hyperparameters);
  const crossValidated = await performCrossValidation(model, splitData.train);
  
  return crossValidated;
}

// Revenue Generation Flows
export async function processSubscriptionRevenue(customer: Customer): Promise<RevenueEvent> {
  const usage = await calculateMonthlyUsage(customer);
  const baseSubscription = await calculateBaseSubscription(customer.tier);
  const overageCharges = await calculateOverageCharges(usage);
  const invoice = await generateInvoice(customer, baseSubscription, overageCharges);
  
  return {
    customerId: customer.id,
    type: 'subscription',
    amount: invoice.total,
    period: invoice.period,
    status: 'billed'
  };
}

export async function processPilotRevenue(pilot: PilotProposal): Promise<RevenueEvent> {
  const milestones = await definePilotMilestones(pilot);
  const completedMilestones = await trackMilestoneCompletion(milestones);
  const billingAmount = await calculateMilestoneBilling(completedMilestones);
  
  return {
    pilotId: pilot.id,
    type: 'pilot',
    amount: billingAmount,
    milestones: completedMilestones,
    status: 'earned'
  };
}

export async function expandCustomerRevenue(customer: Customer): Promise<ExpansionOpportunity> {
  const usageAnalysis = await analyzeCustomerUsage(customer);
  const additionalAssays = await identifyAdditionalAssays(customer);
  const additionalTeams = await identifyAdditionalTeams(customer);
  const tierUpgrade = await assessTierUpgrade(customer, usageAnalysis);
  
  return {
    customerId: customer.id,
    opportunities: [
      ...additionalAssays,
      ...additionalTeams,
      ...(tierUpgrade ? [tierUpgrade] : [])
    ],
    potentialARR: calculateExpansionARR(additionalAssays, additionalTeams, tierUpgrade)
  };
}

// Operational Procedures
export async function executeTriageCampaign(campaign: HTSCampaign): Promise<TriageResults> {
  try {
    await validateCampaignData(campaign);
    const model = await selectOrTrainModel(campaign);
    const predictions = await generatePredictions(model, campaign);
    const rankedResults = await rankAndFilterResults(predictions, campaign.confidenceThreshold);
    const report = await generateTriageReport(rankedResults, model);
    
    await notifyCustomerCompletion(campaign.customerId, report);
    
    return {
      campaignId: campaign.id,
      rankedCompounds: rankedResults.compounds,
      modelMetrics: rankedResults.metrics,
      applicabilityDomain: rankedResults.adMetrics,
      reportUrl: report.url
    };
  } catch (error) {
    await handleTriageError(campaign, error);
    throw error;
  }
}

export async function monitorModelPerformance(): Promise<ModelHealthReport> {
  const activeModels = await getActiveModels();
  const performanceMetrics = await Promise.all(
    activeModels.map(model => evaluateModelPerformance(model))
  );
  
  const driftDetection = await detectModelDrift(activeModels);
  const retrainingNeeded = identifyRetrainingCandidates(performanceMetrics, driftDetection);
  
  if (retrainingNeeded.length > 0) {
    await scheduleModelRetraining(retrainingNeeded);
  }
  
  return {
    totalModels: activeModels.length,
    healthyModels: performanceMetrics.filter(m => m.healthy).length,
    driftingModels: driftDetection.filter(d => d.isDrifting).length,
    retrainingScheduled: retrainingNeeded.length
  };
}

export async function maintainDataQuality(): Promise<QualityReport> {
  const dataSources = await getActiveDataSources();
  const qualityChecks = await Promise.all(
    dataSources.map(source => performQualityChecks(source))
  );
  
  const issues = qualityChecks.flatMap(check => check.issues);
  if (issues.length > 0) {
    await alertDataQualityIssues(issues);
    await initiateDataCleaning(issues);
  }
  
  return {
    sourcesChecked: dataSources.length,
    issuesFound: issues.length,
    cleaningInitiated: issues.filter(i => i.severity === 'high').length
  };
}

// Decision-Making Workflows
export async function decideModelRetrain(model: QSARModel, performanceMetrics: ModelMetrics): Promise<RetrainDecision> {
  const currentPerformance = performanceMetrics.auroc;
  const historicalPerformance = await getHistoricalPerformance(model.id);
  const performanceDrop = historicalPerformance.baseline - currentPerformance;
  
  const dataVolume = await getNewDataVolume(model.lastTrainDate);
  const domainShift = await detectDomainShift(model);
  
  const shouldRetrain = 
    performanceDrop > 0.05 || // 5% AUROC drop
    dataVolume > model.trainingSize * 0.2 || // 20% more data
    domainShift.severity === 'high';
  
  if (shouldRetrain) {
    const retrainPlan = await createRetrainingPlan(model, {
      performanceDrop,
      dataVolume,
      domainShift
    });
    
    return {
      decision: 'retrain',
      reasoning: `Performance drop: ${performanceDrop.toFixed(3)}, New data: ${dataVolume} compounds, Domain shift: ${domainShift.severity}`,
      plan: retrainPlan,
      priority: calculateRetrainPriority(performanceDrop, domainShift)
    };
  }
  
  return {
    decision: 'maintain',
    reasoning: 'Model performance within acceptable thresholds',
    nextReview: new Date(Date.now() + 30 * 24 * 60 * 60 * 1000) // 30 days
  };
}

export async function decideCustomerTierUpgrade(customer: Customer, usage: UsageMetrics): Promise<UpgradeDecision> {
  const currentLimits = getTierLimits(customer.tier);
  const utilizationRate = usage.monthlyCompounds / currentLimits.monthlyCompounds;
  const overageFrequency = usage.overageMonths / 12; // Last 12 months
  
  const shouldUpgrade = 
    utilizationRate > 0.8 || // 80% utilization
    overageFrequency > 0.25; // Overage in >25% of months
  
  if (shouldUpgrade) {
    const nextTier = getNextTier(customer.tier);
    const savings = await calculateUpgradeSavings(customer, usage, nextTier);
    
    return {
      decision: 'recommend-upgrade',
      fromTier: customer.tier,
      toTier: nextTier,
      reasoning: `Utilization: ${(utilizationRate * 100).toFixed(1)}%, Overage frequency: ${(overageFrequency * 100).toFixed(1)}%`,
      projectedSavings: savings,
      urgency: utilizationRate > 0.95 ? 'high' : 'medium'
    };
  }
  
  return {
    decision: 'maintain-tier',
    reasoning: 'Usage within current tier limits',
    nextReview: new Date(Date.now() + 90 * 24 * 60 * 60 * 1000) // 90 days
  };
}

export async function decidePilotSuccess(pilot: PilotExecution, results: TriageResults): Promise<PilotDecision> {
  const successCriteria = pilot.successCriteria;
  const actualMetrics = results.modelMetrics;
  
  const criteriaResults = await Promise.all([
    evaluateCriterion('auroc', actualMetrics.auroc, successCriteria.minAUROC),
    evaluateCriterion('precision', actualMetrics.precision, successCriteria.minPrecision),
    evaluateCriterion('domain_coverage', results.applicabilityDomain.coverage, successCriteria.minDomainCoverage),
    evaluateCriterion('enrichment', await calculateEnrichment(results), successCriteria.minEnrichment)
  ]);
  
  const passedCriteria = criteriaResults.filter(r => r.passed).length;
  const totalCriteria = criteriaResults.length;
  const successRate = passedCriteria / totalCriteria;
  
  if (successRate >= 0.75) { // 75% of criteria met
    const conversionProposal = await generateConversionProposal(pilot, results);
    
    return {
      decision: 'success',
      successRate,
      criteriaResults,
      conversionProposal,
      nextSteps: ['Present results', 'Propose subscription', 'Begin onboarding']
    };
  }
  
  return {
    decision: 'extend-or-modify',
    successRate,
    criteriaResults,
    recommendations: await generateImprovementRecommendations(criteriaResults),
    nextSteps: ['Review with customer', 'Modify approach', 'Extend pilot if needed']
  };
}

// Event-driven workflows using .on() pattern
export const onHTSDataUploaded = async (event: HTSUploadEvent, { ai, db }) => {
  const campaign = await db.campaigns.findById(event.campaignId);
  await validateHTSData(event.data);
  await queueTriageJob(campaign);
  await notifyCustomer(campaign.customerId, 'Data uploaded successfully, triage initiated');
};

export const onTriageCompleted = async (event: TriageCompletedEvent, { ai, db }) => {
  const results = event.results;
  await storeTriageResults(results);
  await generateCustomerReport(results);
  await updateCampaignStatus(event.campaignId, 'completed');
  await trackSuccessMetrics(results);
};

export const onCustomerOnboarded = async (event: CustomerOnboardedEvent, { ai, db }) => {
  const customer = event.customer;
  await setupMonitoring(customer);
  await scheduleHealthChecks(customer);
  await assignCustomerSuccessManager(customer);
  await sendWelcomeSequence(customer);
};

export const onModelDriftDetected = async (event: ModelDriftEvent, { ai, db }) => {
  const model = event.model;
  const severity = event.severity;
  
  if (severity === 'high') {
    await pauseModelInference(model.id);
    await alertModelTeam(model, event);
    await scheduleEmergencyRetrain(model);
  } else {
    await scheduleRoutineRetrain(model);
  }
};

// Scheduled operations using .every() pattern
export const every = {
  // Daily operations
  daily: {
    monitorSystemHealth: async ({ ai, db }) => {
      await checkInfrastructureHealth();
      await validateDataPipelines();
      await reviewActiveJobs();
    },
    
    processUsageMetrics: async ({ ai, db }) => {
      const customers = await db.customers.findActive();
      for (const customer of customers) {
        await updateUsageMetrics(customer);
        await checkUsageLimits(customer);
      }
    }
  },
  
  // Weekly operations
  weekly: {
    reviewModelPerformance: async ({ ai, db }) => {
      const report = await monitorModelPerformance();
      await sendPerformanceReport(report);
    },
    
    analyzeCustomerHealth: async ({ ai, db }) => {
      const customers = await db.customers.findAll();
      for (const customer of customers) {
        const health = await assessCustomerHealth(customer);
        if (health.risk === 'high') {
          await alertCustomerSuccess(customer, health);
        }
      }
    }
  },
  
  // Monthly operations
  monthly: {
    generateRevenueReport: async ({ ai, db }) => {
      const revenue = await calculateMonthlyRevenue();
      const forecast = await generateRevenueForecast();
      await sendRevenueReport(revenue, forecast);
    },
    
    reviewExpansionOpportunities: async ({ ai, db }) => {
      const customers = await db.customers.findAll();
      const opportunities = await Promise.all(
        customers.map(customer => expandCustomerRevenue(customer))
      );
      await prioritizeExpansionOpportunities(opportunities);
    }
  }
};
```

Generated for NAICS 541714 — Research and Development in Biotechnology (except Nanobiotechnology).

export default function Page() {
  return (
    <>
      <Hero
        title="HTS Hit Triage & QSAR Prioritizer"
        subtitle="AI triage for high‑throughput screens. We engineer features, train calibrated QSARs, and rank hits with applicability‑domain awareness—no de novo design required."
        ctaText="Book a demo"
        ctaHref="#demo"
      />
      <Problem bullets={[
        "HTS yields thousands of hits with unclear confirm-worthiness",
        "Plate effects and artifacts inflate false positives", 
        "QSARs break when chemistry drifts outside training space",
        "No uncertainty estimates → costly follow‑ups",
        "Manual feature engineering is slow and inconsistent",
        "Fragmented data formats slow decision‑making"
      ]} />
      <Solution bullets={[
        "End‑to‑end triage that turns raw HTS into ranked, actionable hit lists",
        "Applicability‑domain aware QSAR with calibrated probabilities and abstain outside domain",
        "Automated, assay‑aware feature engineering across fingerprints, descriptors, and GNN embeddings",
        "Robust data QC: plate normalization, replicate handling, and artifact flagging",
        "Transparent model cards and audit‑ready reports for every run",
        "Fast export to CSV/SDF and seamless handoff to confirmatory assays"
      ]} />
      <Features items={[
        "Automated featurization: physicochemical descriptors, circular fingerprints, learned graph embeddings",
        "Data hygiene: control-based plate correction, replicate consensus, PAINS/aggregator flags",
        "Model zoo: RF/XGBoost, elastic nets, GNNs with nested CV and hyperparameter search",
        "Calibration and risk control via conformal prediction; uncertainty and abstention out of domain",
        "Applicability domain via leverage, embedding distance, and ensemble disagreement",
        "Multi‑objective ranking by activity, novelty, diversity, and simple ADMET proxies",
        "Transfer learning across related assays to boost signal with minimal data",
        "Interactive thresholds with expected confirmation rate and cost projections",
        "Reproducible pipelines with versioned data, features, and seeds",
        "Simple integration: SMILES/SDF/SD/CSV in; CSV/SDF out"
      ]} />
      <Steps steps={[
        "Ingest HTS results and structures; define endpoints and assay metadata",
        "QC and normalize: plate/control corrections, replicate reconciliation, artifact checks", 
        "Engineer features and build featurization ensembles",
        "Train, tune, and calibrate models with cross‑validation and holdouts",
        "Score compounds with uncertainty and applicability‑domain tags",
        "Prioritize and export ranked pick lists with cutoffs and rationale"
      ]} />
    </>
  )
}
